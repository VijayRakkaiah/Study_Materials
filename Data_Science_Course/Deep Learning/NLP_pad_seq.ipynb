{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yvXIuTCvtIag"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()# no of words => optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['i love my dog',\n",
        "             'i love my cat',\n",
        "             'you love my dog!',\n",
        "             'do you think my dog is amazing?']\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "metadata": {
        "id": "dfl0mazot-cE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word_index\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVglzgu0uiQH",
        "outputId": "47bc261d-4e61-4d65-9a5d-45b789253ff4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sentences = ['i love my dog',\n",
        "             'i love my cat',\n",
        "             'you love my dog!',\n",
        "             'do you think my dog is amazing?']\n",
        "\n",
        "# padding section\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\") # out of vocab\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfgsdOqSvGjb",
        "outputId": "434d7f2b-e912-41f0-e253-c3d99c7b54d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text to seq\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences,maxlen=6)\n",
        "print(\"word index:\\n\",word_index)\n",
        "print(\"sequences:\\n\",sequences)\n",
        "print(\"padded:\\n\",padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_AeUNiWwoUf",
        "outputId": "f9ff47a4-c6a4-4dc7-8bfa-ffc29281f5ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word index:\n",
            " {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "sequences:\n",
            " [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "padded:\n",
            " [[ 0  0  5  3  2  4]\n",
            " [ 0  0  5  3  2  7]\n",
            " [ 0  0  6  3  2  4]\n",
            " [ 6  9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with unseen words\n",
        "test_data = ['i really love my dog',\n",
        "             'my dog loves my house much']\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"test seq:\\n\",test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXxpJy2bxIEw",
        "outputId": "4d8d4094-9db1-48f0-f3b9-23fcb6465474"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test seq:\n",
            " [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences,maxlen=6, padding = 'pre')\n",
        "print(\"word index:\\n\",word_index)\n",
        "print(\"sequences:\\n\",sequences)\n",
        "print(\"padded:\\n\",padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg54JjXdyTmW",
        "outputId": "7a182d40-c9cb-434c-efaa-9c2695b24a44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word index:\n",
            " {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "sequences:\n",
            " [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "padded:\n",
            " [[ 0  0  5  3  2  4]\n",
            " [ 0  0  5  3  2  7]\n",
            " [ 0  0  6  3  2  4]\n",
            " [ 6  9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CM0RdfEypfM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
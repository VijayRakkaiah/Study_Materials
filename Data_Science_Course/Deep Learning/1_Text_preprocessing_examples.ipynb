{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OumCaqXc_xp"
      },
      "source": [
        "Word Tokenisation\n",
        "============"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht-RuUjbc_xr",
        "outputId": "1062c049-a6f2-47b4-ac16-289c5cbf417b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ],
      "source": [
        "text = \"The quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ6opfM4c_xr",
        "outputId": "627fe2d7-600a-4d51-9bb9-198023bdbdd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox,', 'and', 'an', 'Oxford', 'comma']\n"
          ]
        }
      ],
      "source": [
        "text = \"The quick brown fox, and an Oxford comma\"\n",
        "\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGlL-Oomc_xs",
        "outputId": "7fe45495-d0f7-419f-c2de-0301e0c2c801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqbUrmAoc_xs",
        "outputId": "1ce1662b-45ef-4999-9ef9-0a3a92b57a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', ',', 'and', 'an', 'Oxford', 'comma']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The quick brown fox, and an Oxford comma\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPOgjhcyc_xs",
        "outputId": "0c98a5dd-ddd7-467a-8bb7-48b94518b96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tweet', 'about', '#', 'NLProc', '@', 'PyConUK', ':', ')']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tweet about #NLProc @PyConUK :)\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiKuBh1Mc_xt",
        "outputId": "809697d8-dad0-48b4-91f2-791f12ccf948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tweet', 'about', '#NLProc', '@PyConUK', ':)']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "text = \"Tweet about #NLProc @PyConUK :)\"\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdFJ7wz5c_xt",
        "outputId": "d04a3491-9189-4de1-f060-4fb2cee65238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tweet', 'about', '#NLProc', ':)']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "text = \"Tweet about #NLProc @PyConUK :)\"\n",
        "\n",
        "tokenizer = TweetTokenizer(strip_handles= True)\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AgxY7Ukc_xt",
        "outputId": "0f65f25b-51b3-4607-e1cb-f6e97b097271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['How', 'about', 'currencies', '(', 'like', '£100,000.00', ')', 'and', 'dates', '(', 'like', '19th', 'September', ')']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"How about currencies (like £100,000.00) and dates (like 19th September)\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJRP-I3uc_xt"
      },
      "source": [
        "Stemming\n",
        "====="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03YM4uCQc_xu",
        "outputId": "2a6c6e81-94d3-4b02-fe50-d0b526c175fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n",
            "have\n",
            "had\n",
            "fish\n",
            "fish\n",
            "fisher\n",
            "fish\n",
            "fish\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "s = PorterStemmer()\n",
        "print(s.stem('Having'))\n",
        "print(s.stem('Have'))\n",
        "print(s.stem('Had'))\n",
        "\n",
        "print(s.stem('Fishing'))\n",
        "print(s.stem('Fish'))\n",
        "print(s.stem('Fisher'))\n",
        "print(s.stem('Fishes'))\n",
        "print(s.stem('Fished'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ZkEquMc_xu"
      },
      "source": [
        "Lemmatisation\n",
        "=====\n",
        "\n",
        "Lemmatisation is similar to stemming, as it produces a normalised version of the input word.\n",
        "\n",
        "The output is a lemma, i.e. a proper word (different from stemming)\n",
        "\n",
        "The input word is lemmatised according to its Part-of-Speech (POS) tag, i.e. verb, noun, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2lB4v-Vc_xu",
        "outputId": "4b615b0c-b0a2-4441-94e4-1b004d8f0287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n",
            "have\n",
            "have\n"
          ]
        }
      ],
      "source": [
        "# You'll need the \"wordnet\" package from NLTK data\n",
        "# python -m nltk.downloader wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "s = WordNetLemmatizer()\n",
        "print(s.lemmatize('having', pos='v'))\n",
        "print(s.lemmatize('have', pos='v'))\n",
        "print(s.lemmatize('had', pos='v'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-TuSEjOc_xu",
        "outputId": "29b04bb7-9d3b-4e55-f2d3-7d3b04fd0864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fish\n",
            "fish\n",
            "fisher\n",
            "fish\n",
            "fish\n"
          ]
        }
      ],
      "source": [
        "print(s.lemmatize('fishing', pos='v'))\n",
        "print(s.lemmatize('fish', pos='v'))\n",
        "print(s.lemmatize('fisher', pos='n'))\n",
        "print(s.lemmatize('fishes', pos='v'))\n",
        "print(s.lemmatize('fished', pos='v'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(s.lemmatize('am', pos='v'))\n",
        "print(s.lemmatize('is', pos='v'))\n",
        "print(s.lemmatize('was', pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F4qtWM-ix8I",
        "outputId": "c645c28e-9456-486a-f0b4-56d1942f52d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be\n",
            "be\n",
            "be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipLxzmkzix4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9BvE597bix2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}